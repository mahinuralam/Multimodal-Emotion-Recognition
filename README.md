# TMNet: Transformer-Fused Multimodal Framework for Emotion Recognition

## Overview

TMNet is an advanced multimodal emotion recognition framework that integrates **EEG** (Electroencephalography) signals and **speech signals** for emotion classification. It uses a **Transformer-based fusion** approach to combine outputs from **CNN-BiLSTM** (for speech) and **BiGRU** (for EEG), enhancing the system's ability to capture complex interdependencies between these modalities.

This framework leverages deep learning architectures to process and fuse the features from both EEG and speech signals, resulting in high-performance emotion recognition for applications in **human-computer interaction**, **psychology**, and **healthcare**.

### The main contributions of this work include:
- A **CNN-BiLSTM** model for processing speech signals.
- A **BiGRU** model for EEG signal processing.
- A **Transformer-driven fusion** approach that combines both modalities' outputs to improve emotion recognition accuracy.
