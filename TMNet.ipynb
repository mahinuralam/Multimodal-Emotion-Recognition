{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9443f0-dc16-4918-a130-80cdcf259982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, LSTM, Dense, Dropout, TimeDistributed, Flatten, Bidirectional, BatchNormalization, SpatialDropout1D, MultiHeadAttention, LayerNormalization, Add, GlobalAveragePooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "from scipy.signal import butter, filtfilt, wiener\n",
    "from scipy import signal\n",
    "from sklearn.decomposition import FastICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ea0fe8-ce08-4235-8693-fef1321ae3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "angry_path = os.path.expanduser('/home/mahin/Documents/notebook/Multimodal-Emotion-Recognition/Datasets/Speech/Angry')\n",
    "calm_path = os.path.expanduser('/home/mahin/Documents/notebook/Multimodal-Emotion-Recognition/Datasets/Speech/Calm')\n",
    "neutral_path = os.path.expanduser('/home/mahin/Documents/notebook/Multimodal-Emotion-Recognition/Datasets/Speech/Neutral')\n",
    "happy_path = os.path.expanduser('/home/mahin/Documents/notebook/Multimodal-Emotion-Recognition/Datasets/Speech/Happy')\n",
    "sad_path = os.path.expanduser('/home/mahin/Documents/notebook/Multimodal-Emotion-Recognition/Datasets/Speech/Sad')\n",
    "\n",
    "# Define the number of MFCCs (Mel Frequency Cepstral Coefficients) to extract from each audio file\n",
    "num_mfcc = 40\n",
    "\n",
    "# Define a fixed length for each feature matrix\n",
    "max_len = 500\n",
    "\n",
    "# Track class metadata so downstream cells can stay idempotent\n",
    "emotion_labels = ['Angry', 'Calm', 'Neutral', 'Happy', 'Sad']\n",
    "num_classes = len(emotion_labels)\n",
    "\n",
    "# Define a function to extract the MFCCs from an audio file\n",
    "def extract_features(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=44100)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=num_mfcc)\n",
    "    return mfccs\n",
    "\n",
    "# Load the data into memory\n",
    "X = []\n",
    "y = []\n",
    "speech_file_paths = []\n",
    "for path, emotion in [(angry_path, 0), (calm_path, 1), (neutral_path, 2), (happy_path, 3), (sad_path, 4)]:\n",
    "    for file in os.listdir(path):\n",
    "        file_path = os.path.join(path, file)\n",
    "        if not file_path.lower().endswith(('.wav', '.flac', '.mp3', '.ogg')):\n",
    "            continue\n",
    "        mfccs = extract_features(file_path)\n",
    "        if len(mfccs.T) <= max_len:\n",
    "            pad_width = max_len - len(mfccs.T)\n",
    "            mfccs = np.pad(mfccs.T, pad_width=((0, pad_width), (0, 0)), mode='constant')\n",
    "            X.append(mfccs)\n",
    "            y.append(emotion)\n",
    "            speech_file_paths.append(file_path)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "speech_file_paths = np.array(speech_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c34d9b",
   "metadata": {},
   "source": [
    "## Advanced Multimodal Preprocessing\n",
    "\n",
    "- **Speech**: energy-based voice activity detection (VAD), spectral denoising, MFCC extraction.\n",
    "\n",
    "- **EEG**: band-pass filtering (0.5–45 Hz), FastICA artifact rejection, z-score normalization.\n",
    "\n",
    "- **Synchronization**: Dynamic Time Warping (DTW) to align speech/EEG sequences prior to fusion.\n",
    "\n",
    "- **Fusion**: Transformer-based cross-attention to capture inter-modal dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437924ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def bandpass_filter(data, lowcut=0.5, highcut=45.0, fs=128, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "def run_fast_ica(signal, n_components=8):\n",
    "    length = signal.shape[0]\n",
    "    pad = (n_components - (length % n_components)) % n_components\n",
    "    if pad > 0:\n",
    "        signal = np.pad(signal, (0, pad), mode='edge')\n",
    "    reshaped = signal.reshape(-1, n_components)\n",
    "    ica = FastICA(n_components=n_components, random_state=42, max_iter=500)\n",
    "    try:\n",
    "        sources = ica.fit_transform(reshaped)\n",
    "        reconstructed = ica.inverse_transform(sources)\n",
    "        cleaned = reconstructed.reshape(-1)[:length]\n",
    "    except Exception:\n",
    "        cleaned = signal[:length]\n",
    "    return cleaned\n",
    "\n",
    "def preprocess_eeg_sample(raw_vector, fs=128, window_size=128, step=64):\n",
    "    filtered = bandpass_filter(raw_vector, 0.5, 45.0, fs=fs)\n",
    "    cleaned = run_fast_ica(filtered)\n",
    "    normalized = zscore(cleaned)\n",
    "    if normalized.ndim == 1:\n",
    "        if normalized.size < window_size:\n",
    "            padded = np.pad(normalized, (0, window_size - normalized.size), mode='edge')\n",
    "            windowed = sliding_window_view(padded, window_shape=window_size)[::step]\n",
    "        else:\n",
    "            windowed = sliding_window_view(normalized, window_shape=window_size)[::step]\n",
    "    else:\n",
    "        windowed = sliding_window_view(normalized, window_shape=window_size, axis=0)[::step]\n",
    "    eeg_features = np.stack([windowed.mean(axis=1), windowed.std(axis=1)], axis=-1)\n",
    "    return eeg_features\n",
    "\n",
    "def apply_vad(y, sr, top_db=20):\n",
    "    trimmed, idx = librosa.effects.trim(y, top_db=top_db)\n",
    "    return trimmed if trimmed.size else y\n",
    "\n",
    "def denoise_audio(y):\n",
    "    try:\n",
    "        return wiener(y)\n",
    "    except Exception:\n",
    "        return y\n",
    "\n",
    "def preprocess_speech_file(file_path, target_sr=16000, n_mfcc=40, hop_length=512):\n",
    "    y, sr = librosa.load(file_path, sr=target_sr)\n",
    "    y = librosa.effects.preemphasis(y)\n",
    "    y = denoise_audio(y)\n",
    "    y = apply_vad(y, sr)\n",
    "    if y.size == 0:\n",
    "        y = np.zeros(int(target_sr * 0.5))\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length)\n",
    "    delta = librosa.feature.delta(mfcc)\n",
    "    delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    mfcc_stack = np.stack([mfcc, delta, delta2], axis=-1)\n",
    "    return mfcc_stack.transpose(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad701eb5-6dea-4734-a5bc-d63851e159c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target labels to one-hot encoded vectors without duplicating work on re-runs\n",
    "if 'num_classes' in globals():\n",
    "    expected_classes = num_classes\n",
    "else:\n",
    "    expected_classes = y.shape[1] if y.ndim == 2 else int(np.max(y)) + 1\n",
    "    num_classes = expected_classes\n",
    "\n",
    "if y.ndim == 1:\n",
    "    y = to_categorical(y, num_classes=expected_classes)\n",
    "elif y.ndim == 2 and y.shape[1] == expected_classes:\n",
    "    print(f\"ℹ️ Labels already one-hot encoded with {expected_classes} classes; skipping conversion.\")\n",
    "else:\n",
    "    raise ValueError(f\"Unexpected label shape {y.shape}; expected 1D class indices or one-hot with {expected_classes} columns.\")\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Note: The Bidirectional LSTM model expects shape (batch, timesteps, features)\n",
    "# which is (batch, 500, 40) - no need to add extra dimension for LSTM-only model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e806b1b-0995-48d3-87a3-24ae421d131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels are already one-hot encoded and the train/test split has been created above.\n",
    "# Leaving this cell as a no-op prevents accidentally re-encoding labels twice,\n",
    "# which previously introduced an extra dimension (shape (None, 5, 2))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a00ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation utilities to improve generalization\n",
    "\n",
    "def apply_time_mask(mfcc, max_mask_size=40):\n",
    "    mask_size = np.random.randint(1, max_mask_size)\n",
    "    start = np.random.randint(0, mfcc.shape[0] - mask_size)\n",
    "    mfcc[start:start + mask_size, :] = 0\n",
    "    return mfcc\n",
    "\n",
    "\n",
    "def apply_frequency_mask(mfcc, max_mask_size=8):\n",
    "    mask_size = np.random.randint(1, max_mask_size)\n",
    "    start = np.random.randint(0, mfcc.shape[1] - mask_size)\n",
    "    mfcc[:, start:start + mask_size] = 0\n",
    "    return mfcc\n",
    "\n",
    "\n",
    "def add_gaussian_noise(mfcc, noise_factor=0.015):\n",
    "    noise = np.random.normal(0, noise_factor, mfcc.shape)\n",
    "    return mfcc + noise\n",
    "\n",
    "\n",
    "def augment_mfcc(sample):\n",
    "    augmented = sample.copy()\n",
    "    if random.random() < 0.6:\n",
    "        augmented = add_gaussian_noise(augmented)\n",
    "    if random.random() < 0.5:\n",
    "        augmented = apply_time_mask(augmented)\n",
    "    if random.random() < 0.5:\n",
    "        augmented = apply_frequency_mask(augmented)\n",
    "    return augmented\n",
    "\n",
    "\n",
    "# Augment only the training set to avoid contaminating validation data\n",
    "augmented_samples = np.array([augment_mfcc(sample) for sample in X_train_1], dtype=np.float32)\n",
    "X_train_augmented = np.concatenate([X_train_1.astype(np.float32), augmented_samples], axis=0)\n",
    "y_train_augmented = np.concatenate([y_train_1, y_train_1], axis=0)\n",
    "\n",
    "# Shuffle augmented dataset to mix original and augmented samples\n",
    "shuffle_indices = np.arange(X_train_augmented.shape[0])\n",
    "np.random.shuffle(shuffle_indices)\n",
    "X_train_augmented = X_train_augmented[shuffle_indices]\n",
    "y_train_augmented = y_train_augmented[shuffle_indices]\n",
    "\n",
    "print(f\"✅ Data augmentation complete: {X_train_augmented.shape[0]} training samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6204ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity-check shapes after augmentation to ensure labels remain 2D\n",
    "assert y_train_augmented.ndim == 2 and y_train_augmented.shape[1] == y_train_1.shape[1], (\n",
    "    f\"Unexpected label shape: {y_train_augmented.shape} vs expected ({y_train_1.shape[0]*2}, {y_train_1.shape[1]})\"\n",
    " )\n",
    "print(\"y_train_augmented shape:\", y_train_augmented.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f84c6d5-797f-4e45-acc9-911fccf4eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9121eecb-a5e1-4a94-8a86-76e174290c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(max_len, num_mfcc, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "model.add(Bidirectional(LSTM(units=128)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0308d78e-71b4-48df-9e37-b489ad567c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e2009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for improved model training\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "# TensorBoard callback for visualization\n",
    "tensorboard_callback_improved = TensorBoard(\n",
    "    log_dir='./logs_improved',\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    update_freq='epoch'\n",
    ")\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping_improved = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0.0001,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode='max',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "print(\"✅ Callbacks configured:\")\n",
    "print(\"  📊 TensorBoard: ./logs_improved\")\n",
    "print(\"  ⏹️  Early Stopping: patience=10, monitoring val_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c308065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the improved model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train with 50 epochs (early stopping will stop if no improvement)\n",
    "print(\"🚀 Starting training with 50 epochs...\")\n",
    "print(\"⏱️  This will take ~20-30 minutes\")\n",
    "print(\"📊 Early stopping will save the best model\\n\")\n",
    "\n",
    "history_improved = model.fit(\n",
    "    X_train_1, y_train_1,\n",
    "    validation_data=(X_test_1, y_test_1),\n",
    "    epochs=50,  # Much better than 2 epochs!\n",
    "    batch_size=256,\n",
    "    callbacks=[early_stopping_improved, tensorboard_callback_improved],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the improved model\n",
    "model.save('my_model.h5')\n",
    "print(\"\\n✅ Model saved as 'my_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a39ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "test_loss_improved, test_acc_improved = model.evaluate(X_test_1, y_test_1)\n",
    "\n",
    "print(f'\\n📈 RESULTS:')\n",
    "print(f'  Previous accuracy: 66.08%')\n",
    "print(f'  New accuracy: {test_acc_improved*100:.2f}%')\n",
    "print(f'  Improvement: {(test_acc_improved - 0.6608)*100:.2f}%')\n",
    "print(f'  Test loss: {test_loss_improved:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9b2bdb-bbb7-4458-baf3-06ccfba65dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from tensorflow.keras.models import load_model\n",
    "import seaborn as sns\n",
    "\n",
    "# Predict the classes for the test set\n",
    "y_pred_1 = model.predict(X_test_1)\n",
    "y_pred_classes_1 = np.argmax(y_pred_1, axis=1)  # Convert predictions to class labels\n",
    "y_true_1 = np.argmax(y_test_1, axis=1)  # Convert one-hot true labels to class labels\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true_1, y_pred_classes_1)\n",
    "\n",
    "# Plot confusion matrix using seaborn or matplotlib\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Labels')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.show()\n",
    "\n",
    "# Alternative using ConfusionMatrixDisplay\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "cm_display.plot(cmap='Blues')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbb83ac-08ab-4fed-8cf1-0babf9504b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn import datasets, tree, linear_model, svm\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.layers import GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beb40fc-1523-4d11-9540-5bfa1476bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_csv = os.path.expanduser('/home/mahin/Documents/notebook/Multimodal-Emotion-Recognition/Datasets/EEG/emotions.csv')\n",
    "\n",
    "\n",
    "for dirname, _, filenames in os.walk(path_csv):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "#Reading dataset\n",
    "EEGData = pd.read_csv(path_csv)\n",
    "\n",
    "\n",
    "#! Seprarting Positive,Neagtive and Neutral dataframes for plortting\n",
    "pos = EEGData.loc[EEGData[\"label\"]==\"POSITIVE\"]\n",
    "sample_pos = pos.loc[2, 'fft_0_b':'fft_749_b']\n",
    "neg = EEGData.loc[EEGData[\"label\"]==\"NEGATIVE\"]\n",
    "sample_neg = neg.loc[0, 'fft_0_b':'fft_749_b']\n",
    "neu = EEGData.loc[EEGData[\"label\"]==\"NEUTRAL\"]\n",
    "sample_neu = neu.loc[1, 'fft_0_b':'fft_749_b']\n",
    "\n",
    "def Transform_data(EEGData):\n",
    "    #Encoding Lables into numbers\n",
    "    encoding_data = ({'NEUTRAL': 0, 'POSITIVE': 1, 'NEGATIVE': 2} )\n",
    "    data_encoded = EEGData.replace(encoding_data)\n",
    "    x = data_encoded.drop([\"label\"]  ,axis=1)\n",
    "    y = data_encoded.loc[:,'label'].values\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x)\n",
    "    X_scaled = scaler.transform(x)\n",
    "    X_raw = x.values.astype(np.float32)\n",
    "    Y = to_categorical(y)\n",
    "    return X_scaled, Y, X_raw\n",
    "\n",
    "\n",
    "X_scaled, Y, eeg_raw_signals = Transform_data(EEGData)\n",
    "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(X_scaled, Y, test_size = 0.2, random_state = 4)\n",
    "\n",
    "\n",
    "#! Reshape the data to fit the RNN input shape\n",
    "x_train_2 = np.expand_dims(x_train_2, axis=-1)\n",
    "x_test_2 = np.expand_dims(x_test_2, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b2d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_general_mapping = {\n",
    "    0: 2,  # Angry -> Negative\n",
    "    1: 1,  # Calm -> Positive\n",
    "    2: 0,  # Neutral -> Neutral\n",
    "    3: 1,  # Happy -> Positive\n",
    "    4: 2   # Sad -> Negative\n",
    "}\n",
    "\n",
    "if 'y' not in globals():\n",
    "    raise RuntimeError(\"Speech label array 'y' is required before generating advanced sequences.\")\n",
    "\n",
    "if y.ndim == 2:\n",
    "    label_indices = np.argmax(y, axis=1)\n",
    "else:\n",
    "    label_indices = y.astype(int)\n",
    "\n",
    "advanced_speech_sequences = []\n",
    "advanced_eeg_sequences = []\n",
    "fusion_labels_general = []\n",
    "\n",
    "min_samples = min(len(speech_file_paths), len(eeg_raw_signals))\n",
    "print(f\"Aligning first {min_samples} samples across speech and EEG modalities\")\n",
    "\n",
    "for idx in range(min_samples):\n",
    "    speech_path = speech_file_paths[idx]\n",
    "    speech_feat = preprocess_speech_file(speech_path, target_sr=16000, n_mfcc=40, hop_length=256)\n",
    "    eeg_vector = eeg_raw_signals[idx]\n",
    "    eeg_feat = preprocess_eeg_sample(eeg_vector, fs=128, window_size=128, step=64)\n",
    "    advanced_speech_sequences.append(speech_feat)\n",
    "    advanced_eeg_sequences.append(eeg_feat)\n",
    "    fusion_labels_general.append(speech_general_mapping[int(label_indices[idx])])\n",
    "\n",
    "advanced_speech_sequences = np.array(advanced_speech_sequences, dtype=object)\n",
    "advanced_eeg_sequences = np.array(advanced_eeg_sequences, dtype=object)\n",
    "fusion_labels_general = np.array(fusion_labels_general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2189cccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtw_align_pair(speech_seq: np.ndarray, eeg_seq: np.ndarray):\n",
    "    \"\"\"Align speech and EEG feature sequences using DTW.\"\"\"\n",
    "    speech_arr = np.asarray(speech_seq, dtype=np.float32)\n",
    "    eeg_arr = np.asarray(eeg_seq, dtype=np.float32)\n",
    "\n",
    "    if speech_arr.ndim == 1:\n",
    "        speech_arr = speech_arr[:, None]\n",
    "    elif speech_arr.ndim > 2:\n",
    "        speech_arr = speech_arr.reshape(speech_arr.shape[0], -1)\n",
    "\n",
    "    if eeg_arr.ndim == 1:\n",
    "        eeg_arr = eeg_arr[:, None]\n",
    "    elif eeg_arr.ndim > 2:\n",
    "        eeg_arr = eeg_arr.reshape(eeg_arr.shape[0], -1)\n",
    "\n",
    "    speech_alignment = np.linalg.norm(speech_arr, axis=1, keepdims=True).T\n",
    "    eeg_alignment = np.linalg.norm(eeg_arr, axis=1, keepdims=True).T\n",
    "\n",
    "    _, wp = librosa.sequence.dtw(X=speech_alignment, Y=eeg_alignment, metric=\"euclidean\")\n",
    "    wp = np.array(wp[::-1])\n",
    "\n",
    "    aligned_speech = speech_arr[wp[:, 0]]\n",
    "    aligned_eeg = eeg_arr[wp[:, 1]]\n",
    "    return aligned_speech, aligned_eeg\n",
    "\n",
    "\n",
    "def resample_aligned_sequences(speech_seq: np.ndarray, eeg_seq: np.ndarray, target_len: int):\n",
    "    speech_resampled = signal.resample(speech_seq.astype(np.float32), target_len, axis=0)\n",
    "    eeg_resampled = signal.resample(eeg_seq.astype(np.float32), target_len, axis=0)\n",
    "    return speech_resampled.astype(np.float32), eeg_resampled.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6c41f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_speech_sequences = []\n",
    "dtw_eeg_sequences = []\n",
    "dtw_lengths = []\n",
    "dtw_labels = []\n",
    "skipped_indices = []\n",
    "\n",
    "for idx, (speech_seq, eeg_seq, label) in enumerate(zip(advanced_speech_sequences, advanced_eeg_sequences, fusion_labels_general)):\n",
    "    if speech_seq is None or eeg_seq is None:\n",
    "        skipped_indices.append(idx)\n",
    "        continue\n",
    "    if len(speech_seq) == 0 or len(eeg_seq) == 0:\n",
    "        skipped_indices.append(idx)\n",
    "        continue\n",
    "    try:\n",
    "        aligned_speech, aligned_eeg = dtw_align_pair(speech_seq, eeg_seq)\n",
    "        if aligned_speech.shape[0] < 4 or aligned_eeg.shape[0] < 4:\n",
    "            skipped_indices.append(idx)\n",
    "            continue\n",
    "        dtw_speech_sequences.append(aligned_speech)\n",
    "        dtw_eeg_sequences.append(aligned_eeg)\n",
    "        dtw_lengths.append(aligned_speech.shape[0])\n",
    "        dtw_labels.append(label)\n",
    "    except Exception as err:\n",
    "        print(f\"DTW failed on index {idx}: {err}\")\n",
    "        skipped_indices.append(idx)\n",
    "\n",
    "\n",
    "dtw_lengths = np.array(dtw_lengths)\n",
    "print(f\"DTW alignment complete. Successful pairs: {len(dtw_speech_sequences)} | Skipped: {len(skipped_indices)}\")\n",
    "if len(dtw_lengths) > 0:\n",
    "    suggested_length = int(np.clip(np.percentile(dtw_lengths, 75), 80, 320))\n",
    "else:\n",
    "    suggested_length = 120\n",
    "print(f\"Suggested target sequence length for resampling: {suggested_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4665ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sequence_length = suggested_length\n",
    "resampled_speech_sequences = []\n",
    "resampled_eeg_sequences = []\n",
    "resampled_labels = []\n",
    "\n",
    "for speech_seq, eeg_seq, label in zip(dtw_speech_sequences, dtw_eeg_sequences, dtw_labels):\n",
    "    speech_resampled, eeg_resampled = resample_aligned_sequences(speech_seq, eeg_seq, target_sequence_length)\n",
    "    resampled_speech_sequences.append(speech_resampled)\n",
    "    resampled_eeg_sequences.append(eeg_resampled)\n",
    "    resampled_labels.append(label)\n",
    "\n",
    "resampled_speech_sequences = np.array(resampled_speech_sequences, dtype=np.float32)\n",
    "resampled_eeg_sequences = np.array(resampled_eeg_sequences, dtype=np.float32)\n",
    "resampled_labels = np.array(resampled_labels)\n",
    "\n",
    "print(\"Speech sequences shape:\", resampled_speech_sequences.shape)\n",
    "print(\"EEG sequences shape:\", resampled_eeg_sequences.shape)\n",
    "print(\"Labels shape:\", resampled_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d979f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_labels_onehot = to_categorical(resampled_labels, num_classes=3)\n",
    "\n",
    "fusion_speech_train, fusion_speech_val, fusion_eeg_train, fusion_eeg_val, fusion_y_train, fusion_y_val = train_test_split(\n",
    "    resampled_speech_sequences,\n",
    "    resampled_eeg_sequences,\n",
    "    fusion_labels_onehot,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=resampled_labels\n",
    " )\n",
    "\n",
    "print(\"Fusion train speech shape:\", fusion_speech_train.shape)\n",
    "print(\"Fusion val speech shape:\", fusion_speech_val.shape)\n",
    "print(\"Fusion train EEG shape:\", fusion_eeg_train.shape)\n",
    "print(\"Fusion val EEG shape:\", fusion_eeg_val.shape)\n",
    "print(\"Fusion train labels shape:\", fusion_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f25274",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DTW sequences count:\", len(dtw_speech_sequences))\n",
    "print(\"Skipped indices count:\", len(skipped_indices))\n",
    "print(\"Example advanced speech shape:\", advanced_speech_sequences[0].shape if len(advanced_speech_sequences) else None)\n",
    "print(\"Example EEG shape:\", advanced_eeg_sequences[0].shape if len(advanced_eeg_sequences) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cffd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 0\n",
    "speech_example = advanced_speech_sequences[test_idx]\n",
    "eeg_example = advanced_eeg_sequences[test_idx]\n",
    "speech_arr = np.asarray(speech_example)\n",
    "eeg_arr = np.asarray(eeg_example)\n",
    "if speech_arr.ndim == 1:\n",
    "    speech_arr_proc = speech_arr[:, None]\n",
    "elif speech_arr.ndim > 2:\n",
    "    speech_arr_proc = speech_arr.reshape(speech_arr.shape[0], -1)\n",
    "else:\n",
    "    speech_arr_proc = speech_arr\n",
    "if eeg_arr.ndim == 1:\n",
    "    eeg_arr_proc = eeg_arr[:, None]\n",
    "elif eeg_arr.ndim > 2:\n",
    "    eeg_arr_proc = eeg_arr.reshape(eeg_arr.shape[0], -1)\n",
    "else:\n",
    "    eeg_arr_proc = eeg_arr\n",
    "print(\"Processed speech shape:\", speech_arr_proc.shape)\n",
    "print(\"Processed EEG shape:\", eeg_arr_proc.shape)\n",
    "print(\"Transposed speech shape:\", speech_arr_proc.T.shape)\n",
    "print(\"Transposed EEG shape:\", eeg_arr_proc.T.shape)\n",
    "try:\n",
    "    test_aligned_speech, test_aligned_eeg = dtw_align_pair(speech_example, eeg_example)\n",
    "    print(\"Test aligned shapes:\", test_aligned_speech.shape, test_aligned_eeg.shape)\n",
    "except Exception as err:\n",
    "    print(\"DTW error for test sample:\", err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48657211-272f-44a1-b70d-1a01c6563842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_improved_model():\n",
    "    # Input layer of model for brain signals\n",
    "    inputs = tf.keras.Input(shape=(x_train_2.shape[1], x_train_2.shape[2]))\n",
    "    \n",
    "    # Add Bidirectional GRU layers directly on the 2D sequence input\n",
    "    gru = Bidirectional(GRU(64, return_sequences=True))(inputs)\n",
    "    gru = BatchNormalization()(gru)\n",
    "    gru = Dropout(0.3)(gru)  # Dropout for regularization\n",
    "    \n",
    "    gru = Bidirectional(GRU(128, return_sequences=True))(gru)\n",
    "    gru = BatchNormalization()(gru)\n",
    "    gru = Dropout(0.3)(gru)\n",
    "    \n",
    "    gru = Bidirectional(GRU(64, return_sequences=False))(gru)\n",
    "    gru = BatchNormalization()(gru)\n",
    "    gru = Dropout(0.3)(gru)\n",
    "    \n",
    "    # Flatten the GRU output\n",
    "    flatten = Flatten()(gru)\n",
    "    \n",
    "    # Fully connected layer with ReLU activation\n",
    "    dense = Dense(128, activation='relu')(flatten)\n",
    "    dense = Dropout(0.3)(dense)  # Dropout for regularization\n",
    "    \n",
    "    # Output layer (3 classes: Neutral, Positive, Negative) with softmax activation\n",
    "    outputs = Dense(3, activation='softmax')(dense)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f382af-1aa1-4fb0-9fe0-04312a814f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a callback for TensorBoard\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs_2')\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10, verbose=1, mode='auto',\n",
    "                               baseline=None, restore_best_weights=True)\n",
    "\n",
    "#cretaing model\n",
    "grumodel = create_improved_model()\n",
    "#Compiling model\n",
    "grumodel.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b33198-4b8f-4ed9-a5ce-a653adfafc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = grumodel.fit(\n",
    "    x_train_2, y_train_2,\n",
    "    validation_data=(x_test_2, y_test_2),\n",
    "    epochs=50, \n",
    "    batch_size=256,\n",
    "    callbacks=[early_stopping, tensorboard_callback]\n",
    "  )\n",
    "\n",
    "#Save the model\n",
    "grumodel.save('my_model_gru.h5')\n",
    "\n",
    "#Evaluate the model on test data\n",
    "test_loss, test_acc = grumodel.evaluate(x_test_2, y_test_2)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b002df-4899-4712-8f24-44bc02709ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from tensorflow.keras.models import load_model\n",
    "import seaborn as sns\n",
    "\n",
    "# Predict the classes for the test set\n",
    "y_pred = grumodel.predict(x_test_2)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test_2, axis=1)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "# Plot confusion matrix using seaborn or matplotlib\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Labels')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.show()\n",
    "\n",
    "# Alternative using ConfusionMatrixDisplay\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "cm_display.plot(cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986131c-3dd3-48da-8868-0af509836a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate, Dense, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "def clone_with_prefix(base_model, prefix):\n",
    "    cloned = tf.keras.models.clone_model(base_model)\n",
    "    cloned.set_weights(base_model.get_weights())\n",
    "    for layer in cloned.layers:\n",
    "        layer._name = f\"{prefix}_{layer.name}\"\n",
    "    cloned.trainable = False\n",
    "    return cloned\n",
    "\n",
    "# Use the already trained speech and EEG subnetworks\n",
    "speech_model = clone_with_prefix(model, \"speech\")  # Speech CNN-LSTM\n",
    "eeg_model = clone_with_prefix(grumodel, \"eeg\")     # EEG BiGRU\n",
    "\n",
    "# Functional inputs for each modality\n",
    "speech_input = Input(shape=speech_model.input_shape[1:], name=\"speech_input\")\n",
    "eeg_input = Input(shape=eeg_model.input_shape[1:], name=\"eeg_input\")\n",
    "\n",
    "# Forward pass through frozen subnetworks to obtain logits\n",
    "speech_output = speech_model(speech_input)\n",
    "eeg_output = eeg_model(eeg_input)\n",
    "\n",
    "# Fuse logits/features and learn a lightweight classifier\n",
    "combined = Concatenate(name=\"fusion_concat\")([speech_output, eeg_output])\n",
    "fusion_hidden = Dense(256, activation='relu', name=\"fusion_dense\")(combined)\n",
    "fusion_hidden = Dropout(0.3, name=\"fusion_dropout\")(fusion_hidden)\n",
    "final_output = Dense(y_train_1.shape[1], activation='softmax', name=\"fusion_classifier\")(fusion_hidden)\n",
    "\n",
    "multimodal_model = Model(inputs=[speech_input, eeg_input], outputs=final_output, name=\"multimodal_fusion\")\n",
    "multimodal_model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    " )\n",
    "\n",
    "multimodal_model.summary()\n",
    "\n",
    "# Align datasets across modalities by trimming to the minimum available samples\n",
    "min_train = min(len(X_train_1), len(x_train_2))\n",
    "min_test = min(len(X_test_1), len(x_test_2))\n",
    "\n",
    "speech_train = X_train_1[:min_train]\n",
    "speech_test = X_test_1[:min_test]\n",
    "labels_train = y_train_1[:min_train]\n",
    "labels_test = y_test_1[:min_test]\n",
    "\n",
    "eeg_train = x_train_2[:min_train]\n",
    "eeg_test = x_test_2[:min_test]\n",
    "\n",
    "# Expand speech inputs with a channel dimension if required by the CNN-LSTM branch\n",
    "if speech_model.input_shape[-1] == 1 and speech_train.ndim == 3:\n",
    "    speech_train = speech_train[..., np.newaxis]\n",
    "    speech_test = speech_test[..., np.newaxis]\n",
    "\n",
    "fusion_tensorboard = TensorBoard(\n",
    "    log_dir='./logs_multimodal',\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    update_freq='epoch'\n",
    " )\n",
    "\n",
    "fusion_early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=8,\n",
    "    min_delta=1e-3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    " )\n",
    "\n",
    "history_multimodal = multimodal_model.fit(\n",
    "    [speech_train, eeg_train],\n",
    "    labels_train,\n",
    "    validation_data=([speech_test, eeg_test], labels_test),\n",
    "    epochs=40,\n",
    "    batch_size=256,\n",
    "    callbacks=[fusion_early_stopping, fusion_tensorboard],\n",
    "    verbose=1\n",
    " )\n",
    "\n",
    "multimodal_model.save('multimodal_model.keras')\n",
    "\n",
    "test_loss, test_acc = multimodal_model.evaluate([speech_test, eeg_test], labels_test, verbose=0)\n",
    "print('Multimodal test accuracy:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cooolenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
