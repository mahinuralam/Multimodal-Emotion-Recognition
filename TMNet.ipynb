{"cells":[{"cell_type":"code","source":["import os\n","import numpy as np\n","import librosa\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Dense, Input, Concatenate, Average, Flatten, Dropout, Bidirectional, LSTM, GRU\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import FastICA\n","from scipy.spatial.distance import cdist\n","import tensorflow as tf\n","from sklearn.metrics import accuracy_score\n","\n","# Paths to the datasets\n","angry_path = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/Anger')\n","calm_path = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/Calm')\n","neutral_path = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/Neutral')\n","happy_path = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/Happy')\n","sad_path = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/Sad')\n","\n","# Define the number of MFCCs (Mel Frequency Cepstral Coefficients) to extract from each audio file\n","num_mfcc = 40\n","max_len = 500\n","\n","# Define a function to extract the MFCCs from an audio file with noise filtering\n","def extract_features(file_path):\n","    y, sr = librosa.load(file_path, sr=44100)\n","    y = librosa.effects.preemphasis(y)  # Apply preemphasis for noise filtering\n","    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=num_mfcc)\n","    return mfccs\n","\n","# Load the speech data\n","X = []\n","y = []\n","for path, emotion in [(angry_path, 0), (calm_path, 1), (neutral_path, 2), (happy_path, 3), (sad_path, 4)]:\n","    for file in os.listdir(path):\n","        file_path = os.path.join(path, file)\n","        mfccs = extract_features(file_path)\n","        if len(mfccs.T) <= max_len:\n","            pad_width = max_len - len(mfccs.T)\n","            mfccs = np.pad(mfccs.T, pad_width=((0, pad_width), (0, 0)), mode='constant')\n","            X.append(mfccs)\n","            y.append(emotion)\n","\n","X = np.array(X)\n","y = np.array(y)\n","\n","# Convert the target labels to one-hot encoded vectors\n","y = to_categorical(y)\n","\n","# Load EEG data\n","path_csv = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/emotions.csv')\n","EEGData = pd.read_csv(path_csv)\n","\n","# EEG Data Preprocessing: Noise Filtering, Artifact Removal, and Normalization\n","def preprocess_eeg_signal(EEGData):\n","    # Apply ICA for artifact removal\n","    ica = FastICA(n_components=3)\n","    X_eeg = EEGData.drop([\"label\"], axis=1).values\n","    X_ica = ica.fit_transform(X_eeg)\n","\n","    # Normalize the EEG data (Z-score normalization)\n","    scaler = StandardScaler()\n","    X_normalized = scaler.fit_transform(X_ica)\n","\n","    # Encode labels\n","    label_mapping = {'NEUTRAL': 0, 'POSITIVE': 1, 'NEGATIVE': 2}\n","    y = EEGData['label'].map(label_mapping).values\n","    y = to_categorical(y)\n","\n","    return X_normalized, y\n","\n","# Preprocess EEG data\n","X_eeg, Y_eeg = preprocess_eeg_signal(EEGData)\n","\n","# Split the EEG data into training and testing sets\n","x_train_eeg, x_test_eeg, y_train_eeg, y_test_eeg = train_test_split(X_eeg, Y_eeg, test_size=0.2, random_state=4)\n","\n","# Trim the larger dataset to match the smaller dataset\n","min_samples = min(X.shape[0], X_eeg.shape[0])\n","X_trimmed = X[:min_samples]\n","x_train_trimmed = x_train_eeg[:min_samples]\n","y_train_trimmed = y[:min_samples]\n","y_train_eeg_trimmed = y_train_eeg[:min_samples]\n","\n","# Reshape the speech data\n","X_trimmed = np.expand_dims(X_trimmed, axis=-1)\n","\n","# Define the speech model (CNN-BiLSTM)\n","def create_speech_model():\n","    model = Sequential()\n","    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(max_len, num_mfcc, 1)))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(TimeDistributed(Flatten()))\n","    model.add(Dropout(0.5))\n","    model.add(Dense(512, activation='relu'))\n","    model.add(Bidirectional(LSTM(units=128)))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.5))\n","    model.add(Dense(5, activation='softmax'))  # 5 classes for speech\n","    return model\n","\n","# Define the EEG model (BiGRU)\n","def create_eeg_model():\n","    inputs = Input(shape=(x_train_eeg.shape[1],))\n","    expand_dims = tf.expand_dims(inputs, axis=2)\n","    gru = Bidirectional(GRU(64, return_sequences=True))(expand_dims)\n","    gru = BatchNormalization()(gru)\n","    gru = Dropout(0.3)(gru)\n","    gru = Bidirectional(GRU(128, return_sequences=True))(gru)\n","    gru = BatchNormalization()(gru)\n","    gru = Dropout(0.3)(gru)\n","    gru = Bidirectional(GRU(64, return_sequences=False))(gru)\n","    gru = BatchNormalization()(gru)\n","    gru = Dropout(0.3)(gru)\n","    flatten = Flatten()(gru)\n","    dense = Dense(128, activation='relu')(flatten)\n","    dense = Dropout(0.3)(dense)\n","    outputs = Dense(5, activation='softmax')(dense)  # 3 classes for EEG\n","    model = Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","# Define DTW function for dynamic time warping between the two models' feature vectors\n","from dtw import accelerated_dtw\n","\n","# Function to align each speech and EEG vector pair using DTW\n","def align_with_dtw(speech_feats, eeg_feats):\n","    aligned = []\n","    for i in range(len(speech_feats)):\n","        d, cost_matrix, acc_cost_matrix, path = accelerated_dtw(\n","            speech_feats[i].reshape(-1, 1),\n","            eeg_feats[i].reshape(-1, 1),\n","            dist='euclidean'\n","        )\n","        # Align both vectors to the same length (e.g., average path or interpolate)\n","        aligned_speech = speech_feats[i][path[0]]\n","        aligned_eeg = eeg_feats[i][path[1]]\n","        # Average aligned vectors (or concatenate, depending on your goal)\n","        fused = np.concatenate([aligned_speech, aligned_eeg], axis=0)  # shape: (aligned_len * 2,)\n","        aligned.append(fused)\n","    return np.array(aligned)\n","\n","# Combine the CNN-BiLSTM and BiGRU models' output feature vectors using DTW\n","speech_model = create_speech_model()\n","eeg_model = create_eeg_model()\n","\n","# Compile both models\n","speech_model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n","eeg_model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train both models\n","speech_model.fit(X_trimmed, y_train_trimmed, validation_split=0.2, epochs=2, batch_size=16)\n","eeg_model.fit(x_train_trimmed, y_train_eeg_trimmed, validation_split=0.2, epochs=2, batch_size=16)\n","\n","# Get feature vectors from both models\n","speech_output = speech_model.predict(X_trimmed)\n","eeg_output = eeg_model.predict(x_train_trimmed)\n","\n","# Apply Dynamic Time Warping (DTW) to combine the outputs\n","dtw_distance = align_with_dtw(speech_output, eeg_output)\n","\n","combined_output = dtw_distance\n","\n","from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.utils import to_categorical\n","import numpy as np\n","\n","\n","# Reshape for Transformer input\n","fused_features = np.reshape(combined_output, (100, 2, 128))\n","\n","# Define Transformer Fusion Model\n","def transformer_fusion_model(input_shape):\n","    inputs = Input(shape=input_shape)\n","    x = LayerNormalization(epsilon=1e-6)(inputs)\n","    x = MultiHeadAttention(num_heads=4, key_dim=64)(x, x)\n","    x = Dropout(0.1)(x)\n","    x = LayerNormalization(epsilon=1e-6)(x)\n","    x = GlobalAveragePooling1D()(x)\n","    x = Dense(128, activation='relu')(x)\n","    x = Dropout(0.3)(x)\n","    outputs = Dense(5, activation='softmax')(x)  # Assuming 5-class output\n","    return Model(inputs, outputs)\n","\n","# Compile the model\n","fusion_model = transformer_fusion_model((2, 128))\n","fusion_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Dummy labels for demonstration\n","y_dummy = to_categorical(np.random.randint(0, 5, size=(100,)), num_classes=5)\n","\n","# Train\n","fusion_model.fit(fused_features, y_dummy, epochs=25, batch_size=16, validation_split=0.2)\n","\n","# Predict and evaluate\n","preds = fusion_model.predict(fused_features)\n","pred_labels = np.argmax(preds, axis=1)\n","true_labels = np.argmax(y_dummy, axis=1)\n","\n","from sklearn.metrics import accuracy_score\n","acc = accuracy_score(true_labels, pred_labels)\n","print(f\"Accuracy: {acc:.4f}\")\n"],"metadata":{"id":"5E1MkniJmwud"},"id":"5E1MkniJmwud","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}