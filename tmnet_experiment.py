# -*- coding: utf-8 -*-
"""TMNet_experiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bx0Fwf1pgYPYA_kXkrwOTOfOPUp-Qa09
"""

import os
import numpy as np
import librosa
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, LSTM, Dense, Dropout, TimeDistributed, Flatten, Bidirectional, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, TensorBoard
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical

import tensorflow as tf
tf.keras.backend.clear_session()

angry_path = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/Anger')
calm_path = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/Calm')
neutral_path = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/Neutral')
happy_path = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/Happy')
sad_path = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/Sad')

# Define the number of MFCCs (Mel Frequency Cepstral Coefficients) to extract from each audio file
num_mfcc = 40

# Define a fixed length for each feature matrix
max_len = 500

# Define a function to extract the MFCCs from an audio file
def extract_features(file_path):
    y, sr = librosa.load(file_path, sr=44100)
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=num_mfcc)
    return mfccs

# Load the data into memory
X = []
y = []
for path, emotion in [(angry_path, 0), (calm_path, 1), (neutral_path, 2), (happy_path, 3), (sad_path, 4)]:
    for file in os.listdir(path):
        file_path = os.path.join(path, file)
        mfccs = extract_features(file_path)
        if len(mfccs.T) <= max_len:
            pad_width = max_len - len(mfccs.T)
            mfccs = np.pad(mfccs.T, pad_width=((0, pad_width), (0, 0)), mode='constant')
            X.append(mfccs)
            y.append(emotion)

X = np.array(X)
y = np.array(y)



X.shape

y.shape

# Convert the target labels to one-hot encoded vectors
y = to_categorical(y)

X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X, y, test_size=0.2, random_state=42)

# Reshape the data to fit the RNN input shape
X_train = np.expand_dims(X_train, axis=-1)
X_test = np.expand_dims(X_test, axis=-1)

# Convert the target labels to one-hot encoded vectors
y = to_categorical(y)

X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape

# Define the model architecture
model = Sequential()

model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(max_len, num_mfcc, 1)))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))

model.add(TimeDistributed(Flatten()))
model.add(Dropout(0.5))

model.add(Dense(512, activation='relu'))

model.add(Bidirectional(LSTM(units=128)))
model.add(BatchNormalization())
model.add(Dropout(0.5))

model.add(Dense(5, activation='softmax'))

# Define the model architecture
model = Sequential()

model.add(Conv2D(128, (3, 3), activation='relu', padding='same', input_shape=(max_len, num_mfcc, 1)))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))

model.add(TimeDistributed(Flatten()))
model.add(Dropout(0.5))

model.add(Dense(512, activation='relu'))

model.add(Bidirectional(LSTM(units=128)))
model.add(BatchNormalization())
model.add(Dropout(0.5))

model.add(Dense(5, activation='softmax'))

model.summary()

model = Sequential()

# First Bidirectional LSTM layer
model.add(Bidirectional(LSTM(units=128, return_sequences=True, input_shape=(max_len, num_mfcc))))
model.add(BatchNormalization())  # Add batch normalization
model.add(Dropout(0.3))  # Add dropout to prevent overfitting

# Second Bidirectional LSTM layer
model.add(Bidirectional(LSTM(units=64, return_sequences=True)))
model.add(BatchNormalization())
model.add(Dropout(0.3))

# Third Bidirectional LSTM layer
model.add(Bidirectional(LSTM(units=32, return_sequences=False)))
model.add(BatchNormalization())
model.add(Dropout(0.3))

# Dense layer for classification
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.3))

# Output layer (for 5-class classification)
model.add(Dense(5, activation='softmax'))

# Compile the model
lstmmodel = model
model.compile(optimizer=Adam(learning_rate=1e-4),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Define a callback for TensorBoard
tensorboard_callback = TensorBoard(log_dir='./logs')

# Define early stopping
early_stopping = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10, verbose=1, mode='auto',
                               baseline=None, restore_best_weights=True)

# Train the model
history = lstmmodel.fit(
    X_train_1, y_train_1,
    validation_data=(X_test_1, y_test_1),
    epochs=2,
    batch_size=16,
    callbacks=[early_stopping, tensorboard_callback]
  )

#Save the model
lstmmodel.save('my_model.h5')

#Evaluate the model on test data
test_loss_1, test_acc_1 = lstmmodel.evaluate(X_test_1, y_test_1)

print('Test accuracy:', test_acc_1)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from tensorflow.keras.models import load_model
import seaborn as sns

# Predict the classes for the test set
y_pred_1 = lstmmodel.predict(X_test_1)
y_pred_classes_1 = np.argmax(y_pred_1, axis=1)  # Convert predictions to class labels
y_true_1 = np.argmax(y_test_1, axis=1)  # Convert one-hot true labels to class labels

# Generate confusion matrix
cm = confusion_matrix(y_true_1, y_pred_classes_1)

# Plot confusion matrix using seaborn or matplotlib
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix')
plt.ylabel('Actual Labels')
plt.xlabel('Predicted Labels')
plt.show()

# Alternative using ConfusionMatrixDisplay
cm_display = ConfusionMatrixDisplay(confusion_matrix=cm)
cm_display.plot(cmap='Blues')
plt.show()

for i in range(0, len(y_pred_classes)):
    if y_pred_classes[i] != y_test_classes[i]:
        print("Not match")
    else:
        print("matched")

print("ASDAS")

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import tensorflow
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.optimizers import SGD

from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.layers import Embedding
from tensorflow.keras.layers import LSTM

tf.keras.backend.clear_session()
#from sklearn.metrics import plot_confusion_matrix
from sklearn import datasets, tree, linear_model, svm
from sklearn.metrics import confusion_matrix,classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB

import seaborn as sns
from sklearn.metrics import confusion_matrix,classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix
from tensorflow.keras.layers import GRU

path_csv = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/emotions.csv')


for dirname, _, filenames in os.walk(path_csv):
    for filename in filenames:
        print(os.path.join(dirname, filename))

#Reading dataset
EEGData = pd.read_csv(path_csv)


#Seprarting Positive,Neagtive and Neutral dataframes for plortting
pos = EEGData.loc[EEGData["label"]=="POSITIVE"]
sample_pos = pos.loc[2, 'fft_0_b':'fft_749_b']
neg = EEGData.loc[EEGData["label"]=="NEGATIVE"]
sample_neg = neg.loc[0, 'fft_0_b':'fft_749_b']
neu = EEGData.loc[EEGData["label"]=="NEUTRAL"]
sample_neu = neu.loc[1, 'fft_0_b':'fft_749_b']

def Transform_data(EEGData):
    #Encoding Lables into numbers
    encoding_data = ({'NEUTRAL': 0, 'POSITIVE': 1, 'NEGATIVE': 2} )
    data_encoded = EEGData.replace(encoding_data)
    #getting brain signals into x variable
    x=data_encoded.drop(["label"]  ,axis=1)
    #getting labels into y variable
    y = data_encoded.loc[:,'label'].values
    scaler = StandardScaler()
    #scaling Brain Signals
    scaler.fit(x)
    X = scaler.transform(x)
    #One hot encoding Labels
    Y = to_categorical(y)
    return X,Y



#Calling above function and splitting dataset into train and test
X,Y = Transform_data(EEGData)
x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(X, Y, test_size = 0.2, random_state = 4)



# Reshape the data to fit the RNN input shape
x_train_2 = np.expand_dims(x_train_2, axis=-1)
x_test_2 = np.expand_dims(x_test_2, axis=-1)

#Plotting Positive DataFrame with different line style and markers
plt.figure(figsize=(7, 5))
plt.plot(range(len(sample_pos)), sample_pos, linestyle='-', color='blue', markerfacecolor='red', markeredgewidth=2)
# plt.title("Graph of Positive Columns", fontsize=18)
plt.xlabel('Time(ms)', fontsize=12)
plt.ylabel('Amplitude(Hx)', fontsize=12)
plt.grid(True)
plt.savefig('sample_pos.png')  # Save the plot
plt.show()

import pandas as pd

# Smoothing the signal using a rolling average (window size of 10)
smoothed_pos = pd.Series(sample_pos).rolling(window=10).mean()

plt.figure(figsize=(7, 5))
plt.plot(range(len(sample_pos)), smoothed_pos, color='purple', linewidth=2)
plt.title("Graph of Positive Columns (Smoothed)", fontsize=18)
plt.xlabel('Index', fontsize=14)
plt.ylabel('Signal Value', fontsize=14)
plt.grid(True)
plt.show()

#Plotting Positive DataFrame with different line style and markers
plt.figure(figsize=(7, 5))
plt.plot(range(len(sample_neg)), sample_neg, linestyle='-', color='blue', markerfacecolor='red', markeredgewidth=2)
# plt.title("Graph of Positive Columns", fontsize=18)
plt.xlabel('Time(ms)', fontsize=11)
plt.ylabel('Amplitude(Hx)', fontsize=11)
plt.grid(True)
plt.savefig('sample_neg.png')  # Save the plot
plt.show()

#Plotting Positive DataFrame with different line style and markers
plt.figure(figsize=(7, 5))
plt.plot(range(len(sample_neu)), sample_neu, linestyle='-', color='blue', markerfacecolor='red', markeredgewidth=2)
# plt.title("Graph of Positive Columns", fontsize=18)
plt.xlabel('Time(ms)', fontsize=12)
plt.ylabel('Amplitude(Hx)', fontsize=12)
plt.grid(True)
plt.savefig('sample_neu.png')  # Save the plot
plt.show()

X.shape

Y.shape

x_train.shape

def create_model():
    #input layer of model for brain signals
    inputs = tf.keras.Input(shape=(x_train.shape[1],))
    #Hidden Layer for Brain signal using LSTM(GRU)
    expand_dims = tf.expand_dims(inputs, axis=2)

    gru = tf.keras.layers.GRU(64, return_sequences=True)(expand_dims)
    #Flatten Gru layer into vector form (one Dimensional array)
    gru = tf.keras.layers.GRU(256, return_sequences=True)(expand_dims)
    #Flatten Gru layer into vector form (one Dimensional array)
    gru = tf.keras.layers.GRU(64, return_sequences=True)(expand_dims)
    #Flatten Gru layer into vector form (one Dimensional array)
    flatten = tf.keras.layers.Flatten()(gru)
    #output latyer of Model
    outputs = tf.keras.layers.Dense(3, activation='softmax')(flatten)


    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    print(model.summary())
    return model

def create_improved_model():
    # Input layer of model for brain signals
    inputs = tf.keras.Input(shape=(x_train_2.shape[1],))

    # Expand dimensions to make the input compatible with GRU
    expand_dims = tf.expand_dims(inputs, axis=2)

    # Add Bidirectional GRU layers
    gru = Bidirectional(GRU(64, return_sequences=True))(expand_dims)
    gru = BatchNormalization()(gru)
    gru = Dropout(0.3)(gru)  # Dropout for regularization

    gru = Bidirectional(GRU(128, return_sequences=True))(gru)
    gru = BatchNormalization()(gru)
    gru = Dropout(0.3)(gru)

    gru = Bidirectional(GRU(64, return_sequences=False))(gru)
    gru = BatchNormalization()(gru)
    gru = Dropout(0.3)(gru)

    # Flatten the GRU output
    flatten = Flatten()(gru)

    # Fully connected layer with ReLU activation
    dense = Dense(128, activation='relu')(flatten)
    dense = Dropout(0.3)(dense)  # Dropout for regularization

    # Output layer (3 classes: Neutral, Positive, Negative) with softmax activation
    outputs = Dense(3, activation='softmax')(dense)

    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    print(model.summary())
    return model

# Define a callback for TensorBoard
tensorboard_callback = TensorBoard(log_dir='./logs_2')

# Define early stopping
early_stopping = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10, verbose=1, mode='auto',
                               baseline=None, restore_best_weights=True)

#cretaing model
grumodel = create_improved_model()
#Compiling model
grumodel.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model
history = grumodel.fit(
    x_train, y_train,
    validation_data=(x_test, y_test),
    epochs=2,
    batch_size=16,
    callbacks=[early_stopping, tensorboard_callback]
  )

#Save the model
grumodel.save('my_model_gru.h5')

#Evaluate the model on test data
test_loss, test_acc = grumodel.evaluate(x_test, y_test)

print('Test accuracy:', test_acc)

#Loss and Accuracy of model on Testiong Dataset
print(f"Loss on testing: {loss*100}",f"\nAccuracy on Training: {acc*100}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from tensorflow.keras.models import load_model
import seaborn as sns

# Predict the classes for the test set
y_pred = grumodel.predict(x_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

# Generate confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Plot confusion matrix using seaborn or matplotlib
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix')
plt.ylabel('Actual Labels')
plt.xlabel('Predicted Labels')
plt.show()

# Alternative using ConfusionMatrixDisplay
cm_display = ConfusionMatrixDisplay(confusion_matrix=cm)
cm_display.plot(cmap='Blues')
plt.show()

print("pred class -> ", y_pred_classes)

print("test class -> ", y_true)

print("Unique predicted classes:", np.unique(y_pred_classes))

# Check the unique predicted classes
print("Unique test classes:", np.unique(y_true))

# Adjust the target names if necessary, based on the unique classes present in the predictions
target_names = ['NEUTRAL', 'POSITIVE', 'NEGATIVE']  # Modify this list based on actual predictions

from tensorflow.keras.layers import Concatenate, Dense, Input
from tensorflow.keras.models import Model

def rename_layers(model, prefix):
    for layer in model.layers:
        layer._name = prefix + "_" + layer.name
    return model

# Load or define your models
speech_model = lstmmodel  # Speech model
eeg_model = grumodel      # EEG model

# Rename the layers to ensure unique names
speech_model = rename_layers(speech_model, "speech")
eeg_model = rename_layers(eeg_model, "eeg")

# Combine the models as before
speech_output = speech_model.output
eeg_output = eeg_model.output

combined = Concatenate()([speech_output, eeg_output])

# Add a final output layer
final_output = Dense(5, activation='softmax')(combined)

# Define the multimodal model
multimodal_model = Model(inputs=[speech_model.input, eeg_model.input], outputs=final_output)

# Compile the model
multimodal_model.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Print the model summary
print(multimodal_model.summary())


# Train the multimodal model
history = multimodal_model.fit(
    [X_train, x_train],  # Inputs: Speech and EEG data
    y_train,             # Common labels for both modalities
    validation_data=([X_test, x_test], y_test),
    epochs=2,
    batch_size=16,
    callbacks=[early_stopping, tensorboard_callback]
)

# Save the multimodal model
multimodal_model.save('multimodal_model.h5')

# Evaluate the multimodal model on test data
test_loss, test_acc = multimodal_model.evaluate([X_test, x_test], y_test)
print('Multimodal test accuracy:', test_acc)

import os
import numpy as np
import librosa
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout, TimeDistributed, Flatten, Dense, Bidirectional, LSTM, GRU, Input, Concatenate
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.callbacks import TensorBoard, EarlyStopping
from tensorflow.keras.optimizers import Adam

# Paths to the datasets
angry_path = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/Anger')
calm_path = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/Calm')
neutral_path = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/Neutral')
happy_path = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/Happy')
sad_path = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/Sad')

# Define the number of MFCCs (Mel Frequency Cepstral Coefficients) to extract from each audio file
num_mfcc = 40
max_len = 500

# Define a function to extract the MFCCs from an audio file
def extract_features(file_path):
    y, sr = librosa.load(file_path, sr=44100)
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=num_mfcc)
    return mfccs

# Load the speech data
X = []
y = []
for path, emotion in [(angry_path, 0), (calm_path, 1), (neutral_path, 2), (happy_path, 3), (sad_path, 4)]:
    for file in os.listdir(path):
        file_path = os.path.join(path, file)
        mfccs = extract_features(file_path)
        if len(mfccs.T) <= max_len:
            pad_width = max_len - len(mfccs.T)
            mfccs = np.pad(mfccs.T, pad_width=((0, pad_width), (0, 0)), mode='constant')
            X.append(mfccs)
            y.append(emotion)

X = np.array(X)
y = np.array(y)

# Convert the target labels to one-hot encoded vectors
y = to_categorical(y)

# Load EEG data
path_csv = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/emotions.csv')
EEGData = pd.read_csv(path_csv)

def Transform_data(EEGData):
    # Encoding labels into numbers
    encoding_data = ({'NEUTRAL': 0, 'POSITIVE': 1, 'NEGATIVE': 2} )
    data_encoded = EEGData.replace(encoding_data)
    x = data_encoded.drop(["label"], axis=1)
    y = data_encoded.loc[:, 'label'].values
    scaler = StandardScaler()
    scaler.fit(x)
    X = scaler.transform(x)
    Y = to_categorical(y)
    return X, Y

# Split the EEG data into training and testing sets
X_eeg, Y_eeg = Transform_data(EEGData)
x_train_eeg, x_test_eeg, y_train_eeg, y_test_eeg = train_test_split(X_eeg, Y_eeg, test_size=0.2, random_state=4)

# Trim the larger dataset to match the smaller dataset
min_samples = min(X.shape[0], x_train_eeg.shape[0])

# Trim both datasets
X_trimmed = X[:min_samples]
x_train_trimmed = x_train_eeg[:min_samples]
y_train_trimmed = y[:min_samples]  # Assuming the speech data labels
y_train_eeg_trimmed = y_train_eeg[:min_samples]  # EEG labels, trim accordingly

# Reshape the speech data
X_trimmed = np.expand_dims(X_trimmed, axis=-1)

print("Speech data shape:", X_trimmed.shape)
print("EEG data shape:", x_train_trimmed.shape)
print("Labels shape:", y_train_trimmed.shape)
print("Labels shape:", y_train_eeg_trimmed.shape)

# Define the speech model
def create_speech_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(max_len, num_mfcc, 1)))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))
    model.add(TimeDistributed(Flatten()))
    model.add(Dropout(0.5))
    model.add(Dense(512, activation='relu'))
    model.add(Bidirectional(LSTM(units=128)))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    model.add(Dense(5, activation='softmax'))  # 5 classes for speech
    return model

# Define the EEG model
def create_eeg_model():
    inputs = Input(shape=(x_train_eeg.shape[1],))
    expand_dims = tf.expand_dims(inputs, axis=2)
    gru = Bidirectional(GRU(64, return_sequences=True))(expand_dims)
    gru = BatchNormalization()(gru)
    gru = Dropout(0.3)(gru)
    gru = Bidirectional(GRU(128, return_sequences=True))(gru)
    gru = BatchNormalization()(gru)
    gru = Dropout(0.3)(gru)
    gru = Bidirectional(GRU(64, return_sequences=False))(gru)
    gru = BatchNormalization()(gru)
    gru = Dropout(0.3)(gru)
    flatten = Flatten()(gru)
    dense = Dense(128, activation='relu')(flatten)
    dense = Dropout(0.3)(dense)
    outputs = Dense(3, activation='softmax')(dense)  # 3 classes for EEG
    model = Model(inputs=inputs, outputs=outputs)
    return model

# Create both models
speech_model = create_speech_model()
eeg_model = create_eeg_model()

# Compile both models
speech_model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])
eeg_model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])

# Train both models independently
speech_model.fit(X_trimmed, y_train_trimmed, validation_split=0.2, epochs=2, batch_size=16)
eeg_model.fit(x_train_trimmed, y_train_eeg_trimmed, validation_split=0.2, epochs=2, batch_size=16)

# Combine the results from both models
speech_output = speech_model.output
eeg_output = eeg_model.output

# Concatenate the results
combined_output = Concatenate()([speech_output, eeg_output])

# Final classification layer
final_output = Dense(5, activation='softmax')(combined_output)

# Create the multimodal model
multimodal_model = Model(inputs=[speech_model.input, eeg_model.input], outputs=final_output)

# Compile the multimodal model
multimodal_model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])

# Train the multimodal model
multimodal_model.fit([X_trimmed, x_train_trimmed], y_train_trimmed, validation_split=0.2, epochs=2, batch_size=16)

# Evaluate the multimodal model
test_loss, test_acc = multimodal_model.evaluate([X_test, x_test_eeg], y_test_eeg)
print('Multimodal test accuracy:', test_acc)

import os
import numpy as np
import librosa
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import Dense, Input, Concatenate, Average
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import TensorBoard, EarlyStopping

# Paths to the datasets
angry_path = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/Anger')
calm_path = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/Calm')
neutral_path = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/Neutral')
happy_path = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/Happy')
sad_path = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/Sad')

# Define the number of MFCCs (Mel Frequency Cepstral Coefficients) to extract from each audio file
num_mfcc = 40
max_len = 500

# Define a function to extract the MFCCs from an audio file
def extract_features(file_path):
    y, sr = librosa.load(file_path, sr=44100)
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=num_mfcc)
    return mfccs

# Load the speech data
X = []
y = []
for path, emotion in [(angry_path, 0), (calm_path, 1), (neutral_path, 2), (happy_path, 3), (sad_path, 4)]:
    for file in os.listdir(path):
        file_path = os.path.join(path, file)
        mfccs = extract_features(file_path)
        if len(mfccs.T) <= max_len:
            pad_width = max_len - len(mfccs.T)
            mfccs = np.pad(mfccs.T, pad_width=((0, pad_width), (0, 0)), mode='constant')
            X.append(mfccs)
            y.append(emotion)

X = np.array(X)
y = np.array(y)

# Convert the target labels to one-hot encoded vectors
y = to_categorical(y)

# Load EEG data
path_csv = os.path.expanduser('~/Documents/notebook/MMNet/Dataset/emotions.csv')
EEGData = pd.read_csv(path_csv)

def Transform_data(EEGData):
    # Encoding labels into numbers
    encoding_data = ({'NEUTRAL': 0, 'POSITIVE': 1, 'NEGATIVE': 2} )
    data_encoded = EEGData.replace(encoding_data)
    x = data_encoded.drop(["label"], axis=1)
    y = data_encoded.loc[:, 'label'].values
    scaler = StandardScaler()
    scaler.fit(x)
    X = scaler.transform(x)
    Y = to_categorical(y)
    return X, Y

# Split the EEG data into training and testing sets
X_eeg, Y_eeg = Transform_data(EEGData)
x_train_eeg, x_test_eeg, y_train_eeg, y_test_eeg = train_test_split(X_eeg, Y_eeg, test_size=0.2, random_state=4)

# Trim the larger dataset to match the smaller dataset
min_samples = min(X.shape[0], X_eeg.shape[0])

# Trim both datasets
X_trimmed = X[:min_samples]
x_train_trimmed = x_train_eeg[:min_samples]
y_train_trimmed = y[:min_samples]  # Assuming the speech data labels
y_train_eeg_trimmed = y_train_eeg[:min_samples]  # EEG labels, trim accordingly

# Reshape the speech data
X_trimmed = np.expand_dims(X_trimmed, axis=-1)

# Define the speech model
def create_speech_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(max_len, num_mfcc, 1)))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))
    model.add(TimeDistributed(Flatten()))
    model.add(Dropout(0.5))
    model.add(Dense(512, activation='relu'))
    model.add(Bidirectional(LSTM(units=128)))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    model.add(Dense(5, activation='softmax'))  # 5 classes for speech
    return model

# Define the EEG model
def create_eeg_model():
    inputs = Input(shape=(x_train_eeg.shape[1],))
    expand_dims = tf.expand_dims(inputs, axis=2)
    gru = Bidirectional(GRU(64, return_sequences=True))(expand_dims)
    gru = BatchNormalization()(gru)
    gru = Dropout(0.3)(gru)
    gru = Bidirectional(GRU(128, return_sequences=True))(gru)
    gru = BatchNormalization()(gru)
    gru = Dropout(0.3)(gru)
    gru = Bidirectional(GRU(64, return_sequences=False))(gru)
    gru = BatchNormalization()(gru)
    gru = Dropout(0.3)(gru)
    flatten = Flatten()(gru)
    dense = Dense(128, activation='relu')(flatten)
    dense = Dropout(0.3)(dense)
    outputs = Dense(5, activation='softmax')(dense)  # 3 classes for EEG
    model = Model(inputs=inputs, outputs=outputs)
    return model

# Create both models
speech_model = create_speech_model()
eeg_model = create_eeg_model()

# Compile both models
speech_model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])
eeg_model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])

# Train both models independently
speech_model.fit(X_trimmed, y_train_trimmed, validation_split=0.2, epochs=2, batch_size=16)
eeg_model.fit(x_train_trimmed, y_train_eeg_trimmed, validation_split=0.2, epochs=2, batch_size=16)

# Combine the predictions using late fusion (average)
speech_output = speech_model.predict(X_trimmed)
eeg_output = eeg_model.predict(x_train_trimmed)

# Average the predictions
combined_output = (speech_output + eeg_output) / 2

# Evaluate the combined output on test data
speech_test_output = speech_model.predict(X_trimmed)
eeg_test_output = eeg_model.predict(x_test_eeg)

combined_test_output = (speech_test_output + eeg_test_output) / 2

# Calculate accuracy
from sklearn.metrics import accuracy_score
y_pred = np.argmax(combined_test_output, axis=1)
y_true = np.argmax(y_train_trimmed, axis=1)

accuracy = accuracy_score(y_true, y_pred)
print("Late Fusion Test Accuracy:", accuracy)

def emotion_classification_algorithm(Et, Se):
    """
    Implements Algorithm 1 for emotion classification based on general emotion type Et and specific emotion Se.
    :param Et: General Emotion Type (Positive, Negative, Neutral)
    :param Se: Specific Emotion (Happiness, Calmness, Sadness, Anger, Neutral)
    :return: Classified emotion or error message
    """
    if Et == "Positive":
        if Se == "Happiness":
            return "Happiness"
        elif Se == "Calmness":
            return "Calmness"
        else:
            return "Invalid for Positive emotion type"

    elif Et == "Negative":
        if Se == "Sadness":
            return "Sadness"
        elif Se == "Anger":
            return "Anger"
        else:
            return "Invalid for Negative emotion type"

    elif Et == "Neutral":
        if Se == "Neutral":
            return "Neutral"
        else:
            return "Invalid for Neutral emotion type"

    else:
        return "Invalid emotion type"

# Test on 100 or 200 test cases
n_test_cases = 200  # Use either 100 or 200 cases

# Assuming X_test_speech and X_test_eeg contain the test data
X_test_speech_trimmed = X_test[:n_test_cases]
X_test_eeg_trimmed = x_test_eeg[:n_test_cases]

# Get predictions from both models
speech_predictions = np.argmax(speech_model.predict(X_test_speech_trimmed), axis=1)
eeg_predictions = np.argmax(eeg_model.predict(X_test_eeg_trimmed), axis=1)

# Ground truth labels (assuming they are the same for both modalities)
y_test_trimmed = y_test[:n_test_cases]  # Ground truth for 100 or 200 test cases

# Mappings for the general and specific emotion categories
general_emotion_mapping = {
    0: "Neutral",
    1: "Positive",
    2: "Negative"
}

specific_emotion_mapping = {
    0: "Happiness",  # Positive -> Happiness
    1: "Calmness",   # Positive -> Calmness
    2: "Sadness",    # Negative -> Sadness
    3: "Anger",      # Negative -> Anger
    4: "Neutral"     # Neutral -> Neutral
}

# Initialize counters
correct_classifications = 0
incorrect_classifications = 0

for i in range(n_test_cases):
    # Get predicted general emotion types and specific emotion from both models
    predicted_general_emotion = general_emotion_mapping[eeg_predictions[i]]
    predicted_specific_emotion = specific_emotion_mapping[speech_predictions[i]]

    # Classify the emotion using Algorithm 1
    classified_emotion = emotion_classification_algorithm(predicted_general_emotion, predicted_specific_emotion)

    # Check if the classification matches the ground truth
    ground_truth_general = general_emotion_mapping[np.argmax(y_test_trimmed[i])]
    ground_truth_specific = specific_emotion_mapping[np.argmax(y_test_trimmed[i])]

    if classified_emotion == ground_truth_specific:
        correct_classifications += 1
    else:
        incorrect_classifications += 1

# Calculate accuracy
accuracy = correct_classifications / n_test_cases * 100
print(f"Algorithm accuracy on {n_test_cases} test cases: {accuracy}%")



